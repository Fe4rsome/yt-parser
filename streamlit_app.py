import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
import io
import re
import requests
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="YouTube Parser", page_icon="üìâ", layout="centered")

# --- –°–ï–ö–†–ï–¢–´ ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    TG_TOKEN = st.secrets["TELEGRAM_TOKEN"]
    TG_CHAT_ID = st.secrets["TELEGRAM_CHAT_ID"]
    GEMINI_KEY = st.secrets["GEMINI_API_KEY"]
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ Secrets: {e}")
    st.stop()

# --- –§–£–ù–ö–¶–ò–ò –°–í–Ø–ó–ò ---
def send_telegram_message(text):
    url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    try:
        requests.post(url, json={'chat_id': TG_CHAT_ID, 'text': text, 'parse_mode': 'Markdown'})
    except: pass

def send_results_to_telegram(file_data, file_name, ai_text=None):
    # 1. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –§–∞–π–ª (–í—Å–µ–≥–¥–∞)
    try:
        caption = f"üìÇ {file_name}"
        if ai_text:
            caption += "\n\n(–°–º. –æ—Ç—á–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º)"
            
        requests.post(
            f"https://api.telegram.org/bot{TG_TOKEN}/sendDocument", 
            data={'chat_id': TG_CHAT_ID, 'caption': caption}, 
            files={'document': (file_name, file_data)}
        )
    except: pass
    
    # 2. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¢–µ–∫—Å—Ç AI (–¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å)
    if ai_text:
        url_msg = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
        try:
            if len(ai_text) > 4000:
                requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': ai_text[:4000], 'parse_mode': 'Markdown'})
                requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': ai_text[4000:], 'parse_mode': 'Markdown'})
            else:
                requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': ai_text, 'parse_mode': 'Markdown'})
        except: pass

# --- –§–£–ù–ö–¶–ò–Ø –ê–ù–ê–õ–ò–ó–ê ---
def get_ai_summary_lazy(comments_list):
    if not comments_list: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.", None

    text_corpus = "\n".join([str(c['–¢–µ–∫—Å—Ç'])[:400] for c in comments_list[:80]])
    
    prompt = f"""
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ YouTube.
    –û—Ç—á–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º:
    1. üé≠ –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ.
    2. üî• –¢–µ–º—ã —Å–ø–æ—Ä–æ–≤.
    3. üëç –ü–æ–∑–∏—Ç–∏–≤.
    4. üëé –ù–µ–≥–∞—Ç–∏–≤.
    5. üß† –í—ã–≤–æ–¥.
    
    –¢–µ–∫—Å—Ç: {text_corpus}
    """
    
    # –ü–æ—Ä—è–¥–æ–∫ –ø–µ—Ä–µ–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π
    models = ['gemini-2.0-flash', 'gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-pro']
    
    for model in models:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GEMINI_KEY}"
        try:
            response = requests.post(
                url, 
                json={"contents": [{"parts": [{"text": prompt}]}]}, 
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                return response.json()['candidates'][0]['content']['parts'][0]['text'], model
            elif response.status_code == 429:
                continue # –õ–∏–º–∏—Ç, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é
        except: continue
            
    return "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ AI (–≤—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–Ω—è—Ç—ã –∏–ª–∏ –æ—à–∏–±–∫–∞ —Å–µ—Ç–∏).", None

# --- –ü–ê–†–°–ò–ù–ì ---
def get_video_id(url):
    if "v=" in url: return url.split("v=")[1].split("&")[0]
    elif "youtu.be/" in url: return url.split("youtu.be/")[1].split("?")[0]
    return None

def process_videos(api_key, urls):
    youtube = build('youtube', 'v3', developerKey=api_key)
    all_data = []
    file_name = "comments.xlsx"
    
    for i, url in enumerate(urls):
        v_id = get_video_id(url)
        if not v_id: continue
        try:
            vid_req = youtube.videos().list(part="snippet", id=v_id).execute()
            if vid_req['items']:
                title = vid_req['items'][0]['snippet']['title']
                file_name = f"{re.sub(r'[^\w\s-]', '', title)[:30]}.xlsx"
            
            req = youtube.commentThreads().list(part="snippet", videoId=v_id, maxResults=100)
            while req:
                resp = req.execute()
                for item in resp['items']:
                    top = item['snippet']['topLevelComment']['snippet']
                    all_data.append({'–ê–≤—Ç–æ—Ä': top['authorDisplayName'], '–¢–µ–∫—Å—Ç': top['textDisplay']})
                req = youtube.commentThreads().list_next(req, resp)
        except: pass
    return all_data, file_name

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---
st.title("YouTube Parser üõ†Ô∏è")

# 1. –ü–û–õ–ï –î–õ–Ø –°–°–´–õ–ö–ò
raw_urls = st.text_area("–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ:", height=100)

# 2. –†–£–ë–ò–õ–¨–ù–ò–ö AI (–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –í–´–ö–õ–Æ–ß–ï–ù)
use_ai = st.toggle("–ü–æ–¥–∫–ª—é—á–∏—Ç—å AI-–∞–Ω–∞–ª–∏–∑ (Gemini)", value=False)

# 3. –ö–ù–û–ü–ö–ê –ó–ê–ü–£–°–ö–ê
if st.button("–ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É", type="primary"):
    if not raw_urls:
        st.warning("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É")
    else:
        # –≠–¢–ê–ü 1: –°–±–æ—Ä (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ–≥–¥–∞)
        with st.spinner('–°–∫–∞—á–∏–≤–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏...'):
            data, fname = process_videos(API_KEY, raw_urls.split('\n'))
        
        if data:
            summary = None
            
            # –≠–¢–ê–ü 2: AI (–¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Ä—É–±–∏–ª—å–Ω–∏–∫)
            if use_ai:
                with st.spinner('Gemini –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç...'):
                    summary, used_model = get_ai_summary_lazy(data)
                
                if used_model:
                    st.success(f"–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤! (–ú–æ–¥–µ–ª—å: {used_model})")
                    st.markdown(summary)
                else:
                    st.warning(summary) # –í—ã–≤–æ–¥ –æ—à–∏–±–∫–∏, –µ—Å–ª–∏ AI –Ω–µ —Å–º–æ–≥
            else:
                st.info("AI –∞–Ω–∞–ª–∏–∑ –æ—Ç–∫–ª—é—á–µ–Ω. –¢–æ–ª—å–∫–æ Excel.")

            # –≠–¢–ê–ü 3: Excel (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ–≥–¥–∞)
            df = pd.DataFrame(data)
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                df.to_excel(writer, index=False)
            
            # –≠–¢–ê–ü 4: –û—Ç–ø—Ä–∞–≤–∫–∞ (AI —Ç–µ–∫—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å)
            send_results_to_telegram(buffer.getvalue(), fname, summary)
            
            st.download_button("–°–∫–∞—á–∞—Ç—å Excel", buffer.getvalue(), fname)
            
            if not use_ai:
                st.caption("‚úÖ –§–∞–π–ª –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ Telegram.")
