import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
import io
import re
import requests

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="YouTubeComm", page_icon="üì°", layout="centered")

# --- –°–ï–ö–†–ï–¢–´ ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    TG_TOKEN = st.secrets["TELEGRAM_TOKEN"]
    TG_CHAT_ID = st.secrets["TELEGRAM_CHAT_ID"]
    GEMINI_KEY = st.secrets["GEMINI_API_KEY"]
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ Secrets: {e}")
    st.stop()

# --- –°–ï–°–°–ò–Ø ---
if 'processed' not in st.session_state: st.session_state['processed'] = False
if 'ai_verdict' not in st.session_state: st.session_state['ai_verdict'] = None
if 'quota_used' not in st.session_state: st.session_state['quota_used'] = 0

# --- –¢–ï–õ–ï–ì–†–ê–ú ---
def send_to_telegram(file_data, file_name, ai_text=None, quota_info=""):
    try:
        caption = f"üìÇ {file_name}\n‚ÑπÔ∏è {quota_info}"
        if ai_text: caption += "\n\n(‚¨áÔ∏è –ê–Ω–∞–ª–∏–∑ –Ω–∏–∂–µ)"
        requests.post(
            f"https://api.telegram.org/bot{TG_TOKEN}/sendDocument", 
            data={'chat_id': TG_CHAT_ID, 'caption': caption}, 
            files={'document': (file_name, file_data)}
        )
        if ai_text:
            url_msg = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
            if len(ai_text) > 3000:
                chunks = [ai_text[i:i+3000] for i in range(0, len(ai_text), 3000)]
                for chunk in chunks:
                    requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': chunk})
            else:
                requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': ai_text, 'parse_mode': 'Markdown'})
        return True
    except: return False

# --- –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø ---
def get_video_transcript(video_id):
    try:
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['ru', 'en'])
        return " ".join([t['text'] for t in transcript_list])
    except: return None

# --- AI –ê–ù–ê–õ–ò–ó–ê–¢–û–† (–°–¢–†–û–ì–ò–ô –†–ï–ñ–ò–ú) ---
def get_ai_verdict(title, transcript, comments_list, is_deep_scan):
    if not comments_list: return "–ù–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."
    
    limit = 300 if is_deep_scan else 100
    transcript_limit = 20000 if is_deep_scan else 10000
    transcript_text = transcript[:transcript_limit] if transcript else "–°—É–±—Ç–∏—Ç—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
    audience_voice = "\n".join([f"- {str(c['–¢–µ–∫—Å—Ç'])[:300]}" for c in comments_list[:limit]])
    
    prompt = f"""
    –†–æ–ª—å: –¢—ã —Å—Ç—Ä–æ–≥–∏–π, –±–µ—Å–ø—Ä–∏—Å—Ç—Ä–∞—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞—Ç—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω—É—é –æ—Ü–µ–Ω–∫—É –≤–∏–¥–µ–æ.
    
    1. –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –í–ò–î–ï–û:
    –ù–∞–∑–≤–∞–Ω–∏–µ: {title}
    –°–ª–æ–≤–∞ –∞–≤—Ç–æ—Ä–∞: {transcript_text}...
    
    2. –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ò –ó–†–ò–¢–ï–õ–ï–ô (–í—ã–±–æ—Ä–∫–∞ {limit} —à—Ç):
    {audience_voice}
    
    –ò–ù–°–¢–†–£–ö–¶–ò–Ø:
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å. –ò–≥–Ω–æ—Ä–∏—Ä—É–π –µ–¥–∏–Ω–∏—á–Ω—ã–µ –≤—Å–ø–ª–µ—Å–∫–∏ —ç–º–æ—Ü–∏–π, –∏—â–∏ –æ–±—â–∏–π —Ç—Ä–µ–Ω–¥.
    
    –û–¢–ß–ï–¢ (Markdown):
    1. üéØ –í–ï–†–î–ò–ö–¢ (–û—Ü–µ–Ω–∫–∞ 0-10, –≥–¥–µ 0 - –º—É—Å–æ—Ä/–æ–±–º–∞–Ω, 10 - —à–µ–¥–µ–≤—Ä/–ø–æ–ª—å–∑–∞). –ë—É–¥—å —Å—Ç—Ä–æ–≥.
    2. ‚öñÔ∏è –î–ï–¢–ï–ö–¢–û–† –ü–†–ê–í–î–´ (–õ–æ–∂—å vs –ò—Å—Ç–∏–Ω–∞).
    3. üî• –ì–õ–ê–í–ù–´–ï –°–ü–û–†–´.
    4. üß† –í–´–í–û–î.
    """
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å–∞–º—É—é —É–º–Ω—É—é (Pro), –ø–æ—Ç–æ–º –±—ã—Å—Ç—Ä—ã–µ
    models = ['gemini-1.5-pro', 'gemini-2.5-flash', 'gemini-2.0-flash']
    
    for model in models:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GEMINI_KEY}"
        try:
            # –î–û–ë–ê–í–õ–ï–ù –ö–û–ù–§–ò–ì –¢–ï–ú–ü–ï–†–ê–¢–£–†–´ = 0
            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {
                    "temperature": 0.0,
                    "topP": 0.8,
                    "topK": 40
                }
            }
            
            response = requests.post(url, json=payload, headers={"Content-Type": "application/json"})
            
            if response.status_code == 200:
                return response.json()['candidates'][0]['content']['parts'][0]['text']
        except: continue
    return "‚ö†Ô∏è AI –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª."

# --- –°–ë–û–† –û–¢–í–ï–¢–û–í ---
def get_replies_recursive(youtube, parent_id, progress_callback):
    replies = []
    cost = 0
    try:
        req = youtube.comments().list(parentId=parent_id, part="snippet", maxResults=100)
        while req:
            resp = req.execute()
            cost += 1 
            for item in resp['items']:
                replies.append({
                    '–ê–≤—Ç–æ—Ä': item['snippet']['authorDisplayName'],
                    '–¢–µ–∫—Å—Ç': item['snippet']['textDisplay'],
                    '–¢–∏–ø': '–û—Ç–≤–µ—Ç',
                    '–õ–∞–π–∫–∏': item['snippet']['likeCount']
                })
            progress_callback(len(replies))
            if 'nextPageToken' in resp: req = youtube.comments().list_next(req, resp)
            else: break
    except: pass
    return replies, cost

# --- –ü–ê–†–°–ï–† ---
def process_full_data(api_key, url, use_deep_scan):
    youtube = build('youtube', 'v3', developerKey=api_key)
    all_data = []
    file_name = "report.xlsx"
    total_cost = 0 
    
    status_text = st.empty()
    bar = st.progress(0)

    if "v=" in url: v_id = url.split("v=")[1].split("&")[0]
    elif "youtu.be/" in url: v_id = url.split("youtu.be/")[1].split("?")[0]
    else: return [], "Bad Link", "", None, 0

    try:
        vid_req = youtube.videos().list(part="snippet", id=v_id).execute()
        total_cost += 1
        if vid_req['items']:
            title = vid_req['items'][0]['snippet']['title']
            file_name = f"{re.sub(r'[^\w\s-]', '', title)[:30]}.xlsx"
        else: return [], "Video not found", "", None, 1
        
        transcript = get_video_transcript(v_id)
        
        req = youtube.commentThreads().list(part="snippet,replies", videoId=v_id, maxResults=100)
        fetched_count = 0
        while req:
            resp = req.execute()
            total_cost += 1 
            for item in resp['items']:
                top = item['snippet']['topLevelComment']['snippet']
                all_data.append({'–ê–≤—Ç–æ—Ä': top['authorDisplayName'], '–¢–µ–∫—Å—Ç': top['textDisplay'], '–¢–∏–ø': '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π', '–õ–∞–π–∫–∏': top['likeCount']})
                fetched_count += 1
                
                if item['snippet']['totalReplyCount'] > 0:
                    if use_deep_scan:
                        status_text.text(f"üî• Deep Scan... –ö–≤–æ—Ç–∞: {total_cost}")
                        replies, r_cost = get_replies_recursive(youtube, item['id'], lambda x: None)
                        all_data.extend(replies)
                        total_cost += r_cost
                        fetched_count += len(replies)
                    elif 'replies' in item:
                        for r in item['replies']['comments']:
                            all_data.append({'–ê–≤—Ç–æ—Ä': r['snippet']['authorDisplayName'], '–¢–µ–∫—Å—Ç': r['snippet']['textDisplay'], '–¢–∏–ø': '–û—Ç–≤–µ—Ç', '–õ–∞–π–∫–∏': r['snippet']['likeCount']})
                            fetched_count += 1
            
            bar.progress(min(fetched_count % 100, 100), text=f"–°–æ–±—Ä–∞–Ω–æ: {fetched_count} | –ö–≤–æ—Ç–∞: {total_cost}")
            if 'nextPageToken' in resp: req = youtube.commentThreads().list_next(req, resp)
            else: break
            
    except Exception as e: return [], str(e), "", None, total_cost
    
    bar.empty()
    status_text.empty()
    return all_data, file_name, title, transcript, total_cost

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---
st.markdown("<h3 style='text-align: center;'>YouTubeComm</h3>", unsafe_allow_html=True)

# –°–°–´–õ–ö–ê –ù–ê –ö–û–ù–°–û–õ–¨ (–ö–ù–û–ü–ö–ê –í–í–ï–†–•–£)
st.link_button("üìä –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Å—Ç–∞—Ç–æ–∫ –∫–≤–æ—Ç—ã –≤ Google Console", "https://console.cloud.google.com/apis/api/youtube.googleapis.com/quotas")

raw_url = st.text_input("", placeholder="–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ...")

# –ù–ê–°–¢–†–û–ô–ö–ò
with st.container(border=True):
    c1, c2 = st.columns(2)
    with c1: use_ai = st.toggle("ü§ñ –í–∫–ª—é—á–∏—Ç—å AI", value=False)
    with c2: deep_scan = st.toggle("‚ò¢Ô∏è Deep Scan", value=False, help="–ö–∞—á–∞–µ—Ç –≤—Å–µ –æ—Ç–≤–µ—Ç—ã.")

# –ö–ù–û–ü–ö–ê –ò –ò–ù–§–û
btn_col, info_col = st.columns([1.2, 0.8])
with btn_col:
    start_btn = st.button("–ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É", type="primary", use_container_width=True)
with info_col:
    # –ò–ù–î–ò–ö–ê–¢–û–† –ü–û–¢–†–ê–ß–ï–ù–ù–û–ì–û –ó–ê –°–ï–°–°–ò–Æ
    if st.session_state['quota_used'] > 0:
        st.metric(label="–ü–æ—Ç—Ä–∞—á–µ–Ω–æ –∑–∞ —Ä–∞–∑", value=f"{st.session_state['quota_used']} –µ–¥.", delta=f"-{st.session_state['quota_used']}")
    else:
        st.caption("–õ–∏–º–∏—Ç: 10 000 –µ–¥./–¥–µ–Ω—å")

if start_btn:
    if not raw_url: st.warning("–ù–µ—Ç —Å—Å—ã–ª–∫–∏!")
    else:
        st.session_state['ai_verdict'] = None
        st.session_state['processed'] = False
        
        with st.spinner('–ü–∞—Ä—Å–∏–Ω–≥...'):
            data, fname, title, transcript, cost = process_full_data(API_KEY, raw_url, deep_scan)
            st.session_state['quota_used'] = cost
        
        if data:
            df = pd.DataFrame(data)
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='openpyxl') as writer: df.to_excel(writer, index=False)
            
            ai_text = None
            if use_ai:
                with st.spinner('AI –ê–Ω–∞–ª–∏–∑...'):
                    ai_text = get_ai_verdict(title, transcript, data, deep_scan)
                    st.session_state['ai_verdict'] = ai_text
            
            quota_msg = f"–ü–æ—Ç—Ä–∞—á–µ–Ω–æ –∫–≤–æ—Ç—ã: {cost}"
            sent = send_to_telegram(buffer.getvalue(), fname, ai_text, quota_msg)
            
            if sent: st.success("‚úÖ –§–∞–π–ª –≤ Telegram!")
            st.session_state['processed'] = True
            st.rerun()
        else: st.error(f"–û—à–∏–±–∫–∞: {fname}")

if st.session_state['processed'] and st.session_state['ai_verdict']:
    st.divider()
    st.markdown(st.session_state['ai_verdict'])

