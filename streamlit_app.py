import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
import io
import re
import requests

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="YouTube Pro Parser", page_icon="üá∫üá∏", layout="centered")

# --- –°–ï–ö–†–ï–¢–´ ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    TG_TOKEN = st.secrets["TELEGRAM_TOKEN"]
    TG_CHAT_ID = st.secrets["TELEGRAM_CHAT_ID"]
    GEMINI_KEY = st.secrets["GEMINI_API_KEY"] # –°—é–¥–∞ –≤—Å—Ç–∞–≤—å—Ç–µ –∫–ª—é—á –æ—Ç US –∞–∫–∫–∞—É–Ω—Ç–∞
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ Secrets: {e}")
    st.stop()

# --- –§–£–ù–ö–¶–ò–Ø AI (–° –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Pro –º–æ–¥–µ–ª–∏) ---
def get_ai_summary(comments_list):
    if not comments_list: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."

    # –î–ª—è Pro –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –≤–∑—è—Ç—å –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–¥–æ 100 –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤)
    text_corpus = "\n".join([str(c['–¢–µ–∫—Å—Ç'])[:500] for c in comments_list[:100]])
    
    prompt = f"""
    –¢—ã –æ–ø—ã—Ç–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –≤–∏–¥–µ–æ.
    –°–æ—Å—Ç–∞–≤—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:
    1. üé≠ **–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–Ω:** (–ü–æ–∑–∏—Ç–∏–≤/–ù–µ–≥–∞—Ç–∏–≤/–°–∞—Ä–∫–∞–∑–º).
    2. üî• **–ì–ª–∞–≤–Ω—ã–µ —Ç–µ–º—ã –æ–±—Å—É–∂–¥–µ–Ω–∏—è:** (–û —á–µ–º —Å–ø–æ—Ä—è—Ç).
    3. üëç **–ß—Ç–æ —Ö–≤–∞–ª—è—Ç:** (–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∏—á–∏/–º–æ–º–µ–Ω—Ç—ã).
    4. üëé **–ß—Ç–æ —Ä—É–≥–∞—é—Ç:** (–ë–∞–≥–∏/–ü—Ä–æ–±–ª–µ–º—ã/–¶–µ–Ω—É).
    5. üí° **–ò–Ω—Å–∞–π—Ç:** –°–∞–º—ã–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –∏–ª–∏ –Ω–µ–æ–±—ã—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π.
    
    –¢–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤:
    {text_corpus}
    """
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å–∞–º—É—é –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å (Pro), –∑–∞—Ç–µ–º –±—ã—Å—Ç—Ä—É—é (Flash)
    models = ["gemini-1.5-pro", "gemini-1.5-flash"]

    for model in models:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GEMINI_KEY}"
        headers = {"Content-Type": "application/json"}
        payload = {"contents": [{"parts": [{"text": prompt}]}]}
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            if response.status_code == 200:
                return response.json()['candidates'][0]['content']['parts'][0]['text']
        except:
            continue
            
    return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ VPN –∏–ª–∏ –∫–ª—é—á)."

# --- –û–°–¢–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô) ---
def send_to_telegram(file_data, file_name, ai_text):
    url = f"https://api.telegram.org/bot{TG_TOKEN}/sendDocument"
    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–¥–ø–∏—Å–∏
    caption = f"üá∫üá∏ **Pro –ê–Ω–∞–ª–∏–∑:**\n\n{ai_text[:950]}" 
    files = {'document': (file_name, file_data)}
    try:
        requests.post(url, data={'chat_id': TG_CHAT_ID, 'caption': caption, 'parse_mode': 'Markdown'}, files=files)
        return True
    except:
        return False

def get_video_id(url):
    url = url.strip()
    if "v=" in url: return url.split("v=")[1].split("&")[0]
    elif "youtu.be/" in url: return url.split("youtu.be/")[1].split("?")[0]
    return url if len(url) == 11 else None

def get_video_title(youtube, video_id):
    try:
        resp = youtube.videos().list(part="snippet", id=video_id).execute()
        return resp['items'][0]['snippet']['title']
    except:
        return f"Video_{video_id}"

def process_videos(api_key, urls):
    youtube = build('youtube', 'v3', developerKey=api_key)
    all_data = []
    logs = []
    file_name = "comments.xlsx"
    
    for i, url in enumerate(urls):
        v_id = get_video_id(url)
        if not v_id: continue
        if i == 0:
            title = get_video_title(youtube, v_id)
            file_name = f"{re.sub(r'[\\/*?<>|]', '', title)[:50]}.xlsx"
        
        try:
            req = youtube.commentThreads().list(part="snippet,replies", videoId=v_id, maxResults=100)
            while req:
                resp = req.execute()
                for item in resp['items']:
                    top = item['snippet']['topLevelComment']['snippet']
                    all_data.append({'–ê–≤—Ç–æ—Ä': top['authorDisplayName'], '–¢–µ–∫—Å—Ç': top['textDisplay'], '–î–∞—Ç–∞': top['publishedAt']})
                req = youtube.commentThreads().list_next(req, resp)
        except Exception as e:
            logs.append(f"–û—à–∏–±–∫–∞: {e}")
    return all_data, logs, file_name

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---
st.title("YouTube Pro Parser üá∫üá∏")
raw_urls = st.text_area("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫–∏", height=150)

if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å (Pro)", type="primary"):
    if not raw_urls.strip():
        st.warning("–ù–µ—Ç —Å—Å—ã–ª–æ–∫")
    else:
        with st.spinner('Gemini Pro –¥—É–º–∞–µ—Ç...'):
            urls = raw_urls.split('\n')
            data, logs, fname = process_videos(API_KEY, urls)
        
        if data:
            st.subheader("üìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞")
            summary = get_ai_summary(data)
            st.markdown(summary)
            
            df = pd.DataFrame(data)
            df['–¢–µ–∫—Å—Ç'] = df['–¢–µ–∫—Å—Ç'].astype(str).str.replace(r'<[^>]*>', ' ', regex=True)
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                df.to_excel(writer, index=False)
            
            send_to_telegram(buffer.getvalue(), fname, summary)
            st.success("–ì–æ—Ç–æ–≤–æ!")
            st.download_button(f"–°–∫–∞—á–∞—Ç—å {fname}", buffer.getvalue(), fname)
