import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
import io
import re
import requests

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="YouTubeComm", page_icon="üìâ", layout="centered")

# --- –°–ï–ö–†–ï–¢–´ ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    TG_TOKEN = st.secrets["TELEGRAM_TOKEN"]
    TG_CHAT_ID = st.secrets["TELEGRAM_CHAT_ID"]
    GEMINI_KEY = st.secrets["GEMINI_API_KEY"]
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ Secrets: {e}")
    st.stop()

# --- –¢–ï–õ–ï–ì–†–ê–ú ---
def send_results_to_telegram(file_data, file_name, ai_text=None):
    try:
        caption = f"üìÇ {file_name}"
        if ai_text: caption += "\n\n(–û—Ç—á–µ—Ç AI –Ω–∏–∂–µ)"
        requests.post(
            f"https://api.telegram.org/bot{TG_TOKEN}/sendDocument", 
            data={'chat_id': TG_CHAT_ID, 'caption': caption}, 
            files={'document': (file_name, file_data)}
        )
        if ai_text:
            url_msg = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
            if len(ai_text) > 4000:
                requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': ai_text[:4000], 'parse_mode': 'Markdown'})
                requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': ai_text[4000:], 'parse_mode': 'Markdown'})
            else:
                requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': ai_text, 'parse_mode': 'Markdown'})
    except: pass

# --- AI –ê–ù–ê–õ–ò–ó ---
def get_ai_summary(comments_list):
    if not comments_list: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.", None
    
    text_corpus = "\n".join([str(c['–¢–µ–∫—Å—Ç'])[:400] for c in comments_list[:80]])
    
    prompt = f"""
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ YouTube.
    –ù–∞–ø–∏—à–∏ –æ—Ç—á–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º:
    1. üé≠ –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ.
    2. üî• –¢–µ–º—ã —Å–ø–æ—Ä–æ–≤.
    3. üëç –ü–æ–∑–∏—Ç–∏–≤.
    4. üëé –ù–µ–≥–∞—Ç–∏–≤.
    5. üß† –í—ã–≤–æ–¥.
    
    –¢–µ–∫—Å—Ç: {text_corpus}
    """
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ 2.5-flash, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ —Å—Ä–∞–±–æ—Ç–∞–ª–∞
    models = ['gemini-2.5-flash', 'gemini-2.0-flash', 'gemini-1.5-pro', 'gemini-1.5-flash']
    
    last_error = ""
    for model in models:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GEMINI_KEY}"
        try:
            response = requests.post(
                url, 
                json={"contents": [{"parts": [{"text": prompt}]}]}, 
                headers={"Content-Type": "application/json"}
            )
            if response.status_code == 200:
                return response.json()['candidates'][0]['content']['parts'][0]['text'], model
            else:
                last_error = f"{model}: {response.status_code}"
                continue
        except Exception as e:
            last_error = str(e)
            continue
            
    return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ AI: {last_error}", None

# --- –ü–ê–†–°–ò–ù–ì ---
def process_videos(api_key, urls):
    youtube = build('youtube', 'v3', developerKey=api_key)
    all_data = []
    file_name = "comments.xlsx"
    for i, url in enumerate(urls):
        if "v=" in url: v_id = url.split("v=")[1].split("&")[0]
        elif "youtu.be/" in url: v_id = url.split("youtu.be/")[1].split("?")[0]
        else: continue
        try:
            vid_req = youtube.videos().list(part="snippet", id=v_id).execute()
            if vid_req['items']: 
                title = vid_req['items'][0]['snippet']['title']
                file_name = f"{re.sub(r'[^\w\s-]', '', title)[:30]}.xlsx"
            
            req = youtube.commentThreads().list(part="snippet", videoId=v_id, maxResults=100)
            while req:
                resp = req.execute()
                for item in resp['items']: 
                    all_data.append({
                        '–ê–≤—Ç–æ—Ä': item['snippet']['topLevelComment']['snippet']['authorDisplayName'], 
                        '–¢–µ–∫—Å—Ç': item['snippet']['topLevelComment']['snippet']['textDisplay']
                    })
                req = youtube.commentThreads().list_next(req, resp)
        except: pass
    return all_data, file_name

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---

# 1. –ó–ê–ì–û–õ–û–í–û–ö (HTML –¥–ª—è —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞)
st.markdown("<h3 style='text-align: center;'>YouTubeComm</h3>", unsafe_allow_html=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (—á—Ç–æ–±—ã —Ñ–∞–π–ª –Ω–µ –ø—Ä–æ–ø–∞–¥–∞–ª –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ)
if 'excel_data' not in st.session_state:
    st.session_state['excel_data'] = None
if 'file_name' not in st.session_state:
    st.session_state['file_name'] = None

raw_urls = st.text_area("–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ:", height=100)
use_ai = st.toggle("–ü–æ–¥–∫–ª—é—á–∏—Ç—å AI-–∞–Ω–∞–ª–∏–∑", value=False)

# 2. –ö–ù–û–ü–ö–ò –í –û–î–ò–ù –†–Ø–î (–ö–æ–ª–æ–Ω–∫–∏)
col1, col2 = st.columns([1, 1])

with col1:
    # –ö–Ω–æ–ø–∫–∞ –ù–ê–ß–ê–¢–¨
    if st.button("–ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É", type="primary", use_container_width=True):
        if not raw_urls:
            st.warning("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É")
        else:
            with st.spinner('–ü–∞—Ä—Å–∏–Ω–≥...'):
                data, fname = process_videos(API_KEY, raw_urls.split('\n'))
            
            if data:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å —Å–µ—Å—Å–∏–∏
                df = pd.DataFrame(data)
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False)
                
                st.session_state['excel_data'] = buffer.getvalue()
                st.session_state['file_name'] = fname
                
                # AI –ê–Ω–∞–ª–∏–∑
                summary = None
                if use_ai:
                    with st.spinner('–ê–Ω–∞–ª–∏–∑...'):
                        summary, model_used = get_ai_summary(data)
                    if model_used:
                        st.success(f"–ì–æ—Ç–æ–≤–æ! ({model_used})")
                        st.markdown(summary)
                    else:
                        st.error(summary)
                else:
                    st.info("–ì–æ—Ç–æ–≤–æ (–±–µ–∑ AI).")

                # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ –¢–ì
                send_results_to_telegram(st.session_state['excel_data'], fname, summary)
                # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É, —á—Ç–æ–±—ã –∫–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –æ–±–Ω–æ–≤–∏–ª–∞—Å—å
                st.rerun()

with col2:
    # –ö–Ω–æ–ø–∫–∞ –°–ö–ê–ß–ê–¢–¨ (–ü–æ—è–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ñ–∞–π–ª –µ—Å—Ç—å –≤ –ø–∞–º—è—Ç–∏)
    if st.session_state['excel_data']:
        st.download_button(
            label="–°–∫–∞—á–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É",
            data=st.session_state['excel_data'],
            file_name=st.session_state['file_name'],
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="secondary",
            use_container_width=True
        )
    else:
        # –ü—É—Å—Ç–∞—è –∫–Ω–æ–ø–∫–∞ –¥–ª—è —Å–∏–º–º–µ—Ç—Ä–∏–∏ (–Ω–µ–∞–∫—Ç–∏–≤–Ω–∞—è)
        st.button("–°–∫–∞—á–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É", disabled=True, use_container_width=True)
