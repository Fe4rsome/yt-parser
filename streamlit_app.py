import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
import io
import re
import requests # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É –∑–∞–ø—Ä–æ—Å–æ–≤
# import google.generativeai - –ë–û–õ–¨–®–ï –ù–ï –ù–£–ñ–ù–û

# --- –ù–ê–°–¢–†–û–ô–ö–ò –°–¢–†–ê–ù–ò–¶–´ ---
st.set_page_config(page_title="YouTube AI Parser", page_icon="üß†", layout="centered")

# --- –ü–û–õ–£–ß–ï–ù–ò–ï –°–ï–ö–†–ï–¢–û–í ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    TG_TOKEN = st.secrets["TELEGRAM_TOKEN"]
    TG_CHAT_ID = st.secrets["TELEGRAM_CHAT_ID"]
    GEMINI_KEY = st.secrets["GEMINI_API_KEY"]
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ –≤ Secrets: {e}")
    st.stop()

# --- –§–£–ù–ö–¶–ò–ò ---

def get_ai_summary(comments_list):
    """–ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –∫ Gemini —á–µ—Ä–µ–∑ HTTP (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ–≥–¥–∞)"""
    if not comments_list:
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 50 –∫–æ–º–º–µ–Ω—Ç–æ–≤)
    text_corpus = "\n".join([str(c['–¢–µ–∫—Å—Ç'])[:300] for c in comments_list[:50]])
    
    prompt = f"""
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –≤–∏–¥–µ–æ –∏ –Ω–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ:
    1. –û–±—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ.
    2. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã.
    3. –ß—Ç–æ —Ö–≤–∞–ª—è—Ç/—Ä—É–≥–∞—é—Ç.
    
    –¢–µ–∫—Å—Ç:
    {text_corpus}
    """
    
    # –ü–†–Ø–ú–û–ô –ó–ê–ü–†–û–° –ö API (–ú–ò–ù–£–Ø –ë–ò–ë–õ–ò–û–¢–ï–ö–£)
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_KEY}"
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{"parts": [{"text": prompt}]}]
    }
    
    try:
        response = requests.post(url, json=payload, headers=headers)
        
        if response.status_code == 200:
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç JSON –≤—Ä—É—á–Ω—É—é
            return response.json()['candidates'][0]['content']['parts'][0]['text']
        else:
            # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ - –≤—ã–≤–æ–¥–∏–º —Ç–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç Google (–ø–æ–º–æ–∂–µ—Ç –ø–æ–Ω—è—Ç—å –ø—Ä–∏—á–∏–Ω—É)
            return f"–û—à–∏–±–∫–∞ Google API ({response.status_code}): {response.text}"
            
    except Exception as e:
        return f"–°–±–æ–π —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"

def send_to_telegram(file_data, file_name, ai_text):
    url = f"https://api.telegram.org/bot{TG_TOKEN}/sendDocument"
    caption = f"üìä **AI –ê–Ω–∞–ª–∏–∑:**\n{ai_text[:900]}"
    files = {'document': (file_name, file_data)}
    try:
        requests.post(url, data={'chat_id': TG_CHAT_ID, 'caption': caption, 'parse_mode': 'Markdown'}, files=files)
        return True
    except:
        return False

# --- –°–¢–ê–ù–î–ê–†–¢–ù–´–ï –§–£–ù–ö–¶–ò–ò YOUTUBE ---
def get_video_id(url):
    url = url.strip()
    if "v=" in url: return url.split("v=")[1].split("&")[0]
    elif "youtu.be/" in url: return url.split("youtu.be/")[1].split("?")[0]
    return url if len(url) == 11 else None

def get_video_title(youtube, video_id):
    try:
        resp = youtube.videos().list(part="snippet", id=video_id).execute()
        return resp['items'][0]['snippet']['title']
    except:
        return f"Video_{video_id}"

def process_videos(api_key, urls):
    youtube = build('youtube', 'v3', developerKey=api_key)
    all_data = []
    logs = []
    file_name = "comments.xlsx"
    
    for i, url in enumerate(urls):
        v_id = get_video_id(url)
        if not v_id: continue
        if i == 0:
            title = get_video_title(youtube, v_id)
            file_name = f"{re.sub(r'[\\/*?<>|]', '', title)[:50]}.xlsx"
        
        try:
            req = youtube.commentThreads().list(part="snippet,replies", videoId=v_id, maxResults=100)
            while req:
                resp = req.execute()
                for item in resp['items']:
                    top = item['snippet']['topLevelComment']['snippet']
                    all_data.append({'–ê–≤—Ç–æ—Ä': top['authorDisplayName'], '–¢–µ–∫—Å—Ç': top['textDisplay'], '–î–∞—Ç–∞': top['publishedAt']})
                req = youtube.commentThreads().list_next(req, resp)
        except Exception as e:
            logs.append(f"–û—à–∏–±–∫–∞: {e}")
    return all_data, logs, file_name

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---
st.title("YouTube AI Parser üöÄ")
raw_urls = st.text_area("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫–∏ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=150)

if st.button("–ù–∞—á–∞—Ç—å —Å–±–æ—Ä –∏ AI-–∞–Ω–∞–ª–∏–∑", type="primary"):
    if not raw_urls.strip():
        st.warning("–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫–∏!")
    else:
        with st.spinner('–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é...'):
            urls = raw_urls.split('\n')
            data, logs, fname = process_videos(API_KEY, urls)
        
        if data:
            st.subheader("ü§ñ –°–≤–æ–¥–∫–∞ –æ—Ç Gemini AI")
            ai_summary = get_ai_summary(data)
            
            # –ï—Å–ª–∏ –≤–µ—Ä–Ω—É–ª–∞—Å—å –æ—à–∏–±–∫–∞ 400/403, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –µ—ë –∫—Ä–∞—Å–Ω—ã–º
            if "–û—à–∏–±–∫–∞ Google API" in ai_summary:
                st.error(ai_summary)
            else:
                st.info(ai_summary)
            
            df = pd.DataFrame(data)
            df['–¢–µ–∫—Å—Ç'] = df['–¢–µ–∫—Å—Ç'].astype(str).str.replace(r'<[^>]*>', ' ', regex=True)
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                df.to_excel(writer, index=False)
            
            send_to_telegram(buffer.getvalue(), fname, ai_summary)
            st.success("–ì–æ—Ç–æ–≤–æ! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Telegram.")
            st.download_button(f"üì• –°–∫–∞—á–∞—Ç—å {fname}", buffer.getvalue(), fname)
