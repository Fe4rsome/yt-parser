import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
import io
import re
import requests
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="YouTubeComm", page_icon="‚ò¢Ô∏è", layout="centered")

# --- –°–ï–ö–†–ï–¢–´ ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    TG_TOKEN = st.secrets["TELEGRAM_TOKEN"]
    TG_CHAT_ID = st.secrets["TELEGRAM_CHAT_ID"]
    GEMINI_KEY = st.secrets["GEMINI_API_KEY"]
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ Secrets: {e}")
    st.stop()

# --- –ü–ê–ú–Ø–¢–¨ ---
if 'processed' not in st.session_state: st.session_state['processed'] = False
if 'excel_data' not in st.session_state: st.session_state['excel_data'] = None
if 'file_name' not in st.session_state: st.session_state['file_name'] = ""
if 'ai_text' not in st.session_state: st.session_state['ai_text'] = None

# --- –¢–ï–õ–ï–ì–†–ê–ú ---
def send_results_to_telegram(file_data, file_name, ai_text=None):
    try:
        # 1. –§–∞–π–ª
        caption = f"üìÇ {file_name}"
        if ai_text: caption += "\n\n(–ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç ‚Äî —Å–ª–µ–¥—É—é—â–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º)"
        requests.post(
            f"https://api.telegram.org/bot{TG_TOKEN}/sendDocument", 
            data={'chat_id': TG_CHAT_ID, 'caption': caption}, 
            files={'document': (file_name, file_data)}
        )
        # 2. –¢–µ–∫—Å—Ç
        if ai_text:
            url_msg = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
            if len(ai_text) > 3000:
                chunks = [ai_text[i:i+3000] for i in range(0, len(ai_text), 3000)]
                for chunk in chunks:
                    requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': chunk})
            else:
                requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': ai_text, 'parse_mode': 'Markdown'})
    except: pass

# --- AI –ê–ù–ê–õ–ò–ó ---
def get_ai_summary(comments_list):
    if not comments_list: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.", None
    
    # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 120 –∫–æ–º–º–µ–Ω—Ç–æ–≤)
    text_corpus = "\n".join([str(c['–¢–µ–∫—Å—Ç'])[:400] for c in comments_list[:120]])
    
    prompt = f"""
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ YouTube.
    –°–æ—Å—Ç–∞–≤—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:
    1. –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ (–¥–µ—Ç–∞–ª—å–Ω–æ)
    2. –û—Å–Ω–æ–≤–Ω—ã–µ –≤–µ—Ç–∫–∏ —Å–ø–æ—Ä–æ–≤
    3. –ê—Ä–≥—É–º–µ–Ω—Ç—ã "–ó–ê"
    4. –ê—Ä–≥—É–º–µ–Ω—Ç—ã "–ü–†–û–¢–ò–í"
    5. –í—ã–≤–æ–¥ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
    
    –¢–µ–∫—Å—Ç: {text_corpus}
    """
    
    models = ['gemini-2.5-flash', 'gemini-2.0-flash', 'gemini-1.5-pro', 'gemini-1.5-flash']
    
    last_error = ""
    for model in models:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GEMINI_KEY}"
        try:
            response = requests.post(
                url, 
                json={"contents": [{"parts": [{"text": prompt}]}]}, 
                headers={"Content-Type": "application/json"}
            )
            if response.status_code == 200:
                return response.json()['candidates'][0]['content']['parts'][0]['text'], model
            else:
                last_error = f"{model} ({response.status_code})"
        except Exception as e:
            last_error = str(e)
            continue
            
    return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ AI: {last_error}", None

# --- –§–£–ù–ö–¶–ò–Ø: –ü–û–õ–£–ß–ò–¢–¨ –í–°–ï –û–¢–í–ï–¢–´ (–Ø–î–ï–†–ù–´–ô –†–ï–ñ–ò–ú) ---
def get_all_replies(youtube, parent_id):
    replies = []
    try:
        req = youtube.comments().list(parentId=parent_id, part="snippet", maxResults=100)
        while req:
            resp = req.execute()
            for item in resp['items']:
                replies.append({
                    '–ê–≤—Ç–æ—Ä': item['snippet']['authorDisplayName'],
                    '–¢–µ–∫—Å—Ç': item['snippet']['textDisplay'],
                    '–¢–∏–ø': '–û—Ç–≤–µ—Ç',
                    '–õ–∞–π–∫–∏': item['snippet']['likeCount']
                })
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ—Ç–≤–µ—Ç–æ–≤
            if 'nextPageToken' in resp:
                req = youtube.comments().list_next(req, resp)
            else:
                break
    except: pass
    return replies

# --- –ü–ê–†–°–ò–ù–ì ---
def process_videos(api_key, urls, deep_scan=False):
    youtube = build('youtube', 'v3', developerKey=api_key)
    all_data = []
    file_name = "comments.xlsx"
    
    # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, url in enumerate(urls):
        if "v=" in url: v_id = url.split("v=")[1].split("&")[0]
        elif "youtu.be/" in url: v_id = url.split("youtu.be/")[1].split("?")[0]
        else: continue
        
        try:
            vid_req = youtube.videos().list(part="snippet", id=v_id).execute()
            if vid_req['items']: 
                title = vid_req['items'][0]['snippet']['title']
                file_name = f"{re.sub(r'[^\w\s-]', '', title)[:30]}.xlsx"
            
            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ç—Ä–µ–¥—ã
            req = youtube.commentThreads().list(part="snippet,replies", videoId=v_id, maxResults=100)
            
            total_fetched = 0
            
            while req:
                resp = req.execute()
                for item in resp['items']: 
                    # 1. –ì–ª–∞–≤–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
                    top = item['snippet']['topLevelComment']['snippet']
                    all_data.append({
                        '–ê–≤—Ç–æ—Ä': top['authorDisplayName'], 
                        '–¢–µ–∫—Å—Ç': top['textDisplay'],
                        '–¢–∏–ø': '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π',
                        '–õ–∞–π–∫–∏': top['likeCount']
                    })
                    total_fetched += 1
                    
                    # 2. –†–∞–±–æ—Ç–∞ —Å –æ—Ç–≤–µ—Ç–∞–º–∏
                    reply_count = item['snippet']['totalReplyCount']
                    
                    if reply_count > 0:
                        if deep_scan:
                            # –†–ï–ñ–ò–ú "–Ø–î–ï–†–ù–´–ô": –ö–∞—á–∞–µ–º –≤—Å—ë –æ—Ç–¥–µ–ª—å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
                            # –≠—Ç–æ —Ç—Ä–∞—Ç–∏—Ç –∫–≤–æ—Ç—É, –Ω–æ –¥–æ—Å—Ç–∞–µ—Ç –≤—Å–µ –æ—Ç–≤–µ—Ç—ã
                            replies = get_all_replies(youtube, item['id'])
                            all_data.extend(replies)
                            total_fetched += len(replies)
                        else:
                            # –†–ï–ñ–ò–ú "–≠–ö–û–ù–û–ú": –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –¥–∞–ª–∏ —Å—Ä–∞–∑—É (–¥–æ 5 —à—Ç)
                            if 'replies' in item:
                                for reply in item['replies']['comments']:
                                    all_data.append({
                                        '–ê–≤—Ç–æ—Ä': reply['snippet']['authorDisplayName'], 
                                        '–¢–µ–∫—Å—Ç': reply['snippet']['textDisplay'],
                                        '–¢–∏–ø': '–û—Ç–≤–µ—Ç',
                                        '–õ–∞–π–∫–∏': reply['snippet']['likeCount']
                                    })
                                    total_fetched += 1

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                status_text.text(f"–°–æ–±—Ä–∞–Ω–æ: {total_fetched}...")
                
                if 'nextPageToken' in resp:
                    req = youtube.commentThreads().list_next(req, resp)
                else:
                    break
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ API: {e}")
            
    progress_bar.empty()
    status_text.empty()
    return all_data, file_name

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---
st.markdown("<h3 style='text-align: center; margin-bottom: 10px;'>YouTubeComm</h3>", unsafe_allow_html=True)

raw_urls = st.text_area("–°—Å—ã–ª–∫–∞:", height=100)

# –ù–ê–°–¢–†–û–ô–ö–ò (–í –î–í–ï –ö–û–õ–û–ù–ö–ò)
c1, c2 = st.columns(2)
with c1:
    use_ai = st.toggle("AI-–∞–Ω–∞–ª–∏–∑", value=False)
with c2:
    # –¢–û–¢ –°–ê–ú–´–ô –ü–û–õ–ó–£–ù–û–ö
    deep_scan = st.toggle("üî• –í—Å–µ –æ—Ç–≤–µ—Ç—ã (–ú–µ–¥–ª–µ–Ω–Ω–æ!)", value=False, help="–í—ã–≥—Ä—É–∂–∞–µ—Ç –í–°–ï –≤–µ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤. –¢—Ä–∞—Ç–∏—Ç –º–Ω–æ–≥–æ –∫–≤–æ—Ç—ã.")

# –ö–ù–û–ü–ö–ê –ó–ê–ü–£–°–ö–ê
if st.button("–ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É", type="primary", use_container_width=True):
    if not raw_urls:
        st.warning("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É")
    else:
        st.session_state['processed'] = False
        
        with st.spinner('–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...'):
            # –ü–µ—Ä–µ–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä deep_scan
            data, fname = process_videos(API_KEY, raw_urls.split('\n'), deep_scan=deep_scan)
        
        if data:
            df = pd.DataFrame(data)
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                df.to_excel(writer, index=False)
            
            st.session_state['excel_data'] = buffer.getvalue()
            st.session_state['file_name'] = fname
            st.session_state['ai_text'] = None
            
            if use_ai:
                with st.spinner('–ê–Ω–∞–ª–∏–∑...'):
                    summary, mod = get_ai_summary(data)
                    st.session_state['ai_text'] = summary

            st.session_state['processed'] = True
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞
            send_results_to_telegram(st.session_state['excel_data'], fname, st.session_state['ai_text'])

# –ë–õ–û–ö –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
if st.session_state['processed']:
    st.divider()
    
    st.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ–±—Ä–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(pd.read_excel(io.BytesIO(st.session_state['excel_data'])))}")
    
    if st.session_state['ai_text']:
        if "–û—à–∏–±–∫–∞" in st.session_state['ai_text']:
            st.error(st.session_state['ai_text'])
        else:
            st.markdown(st.session_state['ai_text'])
    
    st.download_button(
        label=f"üì• –°–∫–∞—á–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É",
        data=st.session_state['excel_data'],
        file_name=st.session_state['file_name'],
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True
    )
