import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
import io
import re
import requests
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="YouTubeComm", page_icon="üì°", layout="centered")

# --- –°–ï–ö–†–ï–¢–´ ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    TG_TOKEN = st.secrets["TELEGRAM_TOKEN"]
    TG_CHAT_ID = st.secrets["TELEGRAM_CHAT_ID"]
    GEMINI_KEY = st.secrets["GEMINI_API_KEY"]
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ Secrets: {e}")
    st.stop()

# --- –°–ï–°–°–ò–Ø ---
if 'processed' not in st.session_state: st.session_state['processed'] = False
if 'ai_verdict' not in st.session_state: st.session_state['ai_verdict'] = None
if 'quota_used' not in st.session_state: st.session_state['quota_used'] = 0

# --- –¢–ï–õ–ï–ì–†–ê–ú ---
def send_to_telegram(file_data, file_name, ai_text=None, quota_info=""):
    try:
        # 1. –§–∞–π–ª
        caption = f"üìÇ {file_name}\n‚ÑπÔ∏è {quota_info}"
        if ai_text: caption += "\n\n(‚¨áÔ∏è –ê–Ω–∞–ª–∏–∑ –Ω–∏–∂–µ)"
        requests.post(
            f"https://api.telegram.org/bot{TG_TOKEN}/sendDocument", 
            data={'chat_id': TG_CHAT_ID, 'caption': caption}, 
            files={'document': (file_name, file_data)}
        )
        # 2. –¢–µ–∫—Å—Ç
        if ai_text:
            url_msg = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
            if len(ai_text) > 3000:
                chunks = [ai_text[i:i+3000] for i in range(0, len(ai_text), 3000)]
                for chunk in chunks:
                    requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': chunk})
            else:
                requests.post(url_msg, json={'chat_id': TG_CHAT_ID, 'text': ai_text, 'parse_mode': 'Markdown'})
        return True
    except: return False

# --- –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø ---
def get_video_transcript(video_id):
    try:
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['ru', 'en'])
        return " ".join([t['text'] for t in transcript_list])
    except: return None

# --- AI –ê–ù–ê–õ–ò–ó–ê–¢–û–† (–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–∏–¥–µ–æ –∏ –∫–æ–º–º–µ–Ω—Ç–æ–≤) ---
def get_ai_verdict(title, transcript, comments_list, is_deep_scan):
    if not comments_list: return "–ù–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."
    
    # –ï—Å–ª–∏ Deep Scan –≤–∫–ª—é—á–µ–Ω, –¥–∞–µ–º AI –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö (300 –∫–æ–º–º–µ–Ω—Ç–æ–≤ –≤–º–µ—Å—Ç–æ 100)
    limit = 300 if is_deep_scan else 100
    transcript_limit = 20000 if is_deep_scan else 10000
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –≤–∏–¥–µ–æ
    transcript_text = transcript[:transcript_limit] if transcript else "–°—É–±—Ç–∏—Ç—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –º–Ω–µ–Ω–∏–µ –Ω–∞—Ä–æ–¥–∞
    audience_voice = "\n".join([f"- {str(c['–¢–µ–∫—Å—Ç'])[:300]}" for c in comments_list[:limit]])
    
    prompt = f"""
    –†–æ–ª—å: –¢—ã –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ YouTube. 
    –ó–∞–¥–∞—á–∞: –°—Ä–∞–≤–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤–∏–¥–µ–æ (—Å–ª–æ–≤–∞ –∞–≤—Ç–æ—Ä–∞) —Å —Ä–µ–∞–∫—Ü–∏–µ–π –∑—Ä–∏—Ç–µ–ª–µ–π.
    
    1. –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –í–ò–î–ï–û:
    –ù–∞–∑–≤–∞–Ω–∏–µ: {title}
    –°–ª–æ–≤–∞ –∞–≤—Ç–æ—Ä–∞ (—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è): {transcript_text}...
    
    2. –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ò –ó–†–ò–¢–ï–õ–ï–ô (–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø-{limit}):
    {audience_voice}
    
    –°–û–°–¢–ê–í–¨ –û–¢–ß–ï–¢ (Markdown):
    1. üéØ **–í–ï–†–î–ò–ö–¢:** (–°—Ç–æ–∏—Ç —Å–º–æ—Ç—Ä–µ—Ç—å? –û—Ü–µ–Ω–∫–∞ 0-10).
    2. ‚öñÔ∏è **–î–ï–¢–ï–ö–¢–û–† –ü–†–ê–í–î–´:** –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –ª–∏ –∑—Ä–∏—Ç–µ–ª–∏ —Å–ª–æ–≤–∞ –∞–≤—Ç–æ—Ä–∞? –ï—Å—Ç—å –ª–∏ –æ–ø—Ä–æ–≤–µ—Ä–∂–µ–Ω–∏—è –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö?
    3. üî• **–ì–õ–ê–í–ù–´–ï –°–ü–û–†–´:** –û —á–µ–º —Å–∞–º–∞—è –∂–∞—Ä–∫–∞—è –¥–∏—Å–∫—É—Å—Å–∏—è (–æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ—Ç–≤–µ—Ç–∞—Ö).
    4. üß† **–í–´–í–û–î:** –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ.
    """
    
    models = ['gemini-2.5-flash', 'gemini-1.5-pro', 'gemini-2.0-flash', 'gemini-1.5-flash']
    
    for model in models:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GEMINI_KEY}"
        try:
            response = requests.post(
                url, 
                json={"contents": [{"parts": [{"text": prompt}]}]}, 
                headers={"Content-Type": "application/json"}
            )
            if response.status_code == 200:
                return response.json()['candidates'][0]['content']['parts'][0]['text']
        except: continue
    return "‚ö†Ô∏è AI –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª."

# --- –§–£–ù–ö–¶–ò–Ø –°–ë–û–†–ê –û–¢–í–ï–¢–û–í (–†–ï–ö–£–†–°–ò–í–ù–ê–Ø) ---
def get_replies_recursive(youtube, parent_id, progress_callback):
    replies = []
    cost = 0
    try:
        req = youtube.comments().list(parentId=parent_id, part="snippet", maxResults=100)
        while req:
            resp = req.execute()
            cost += 1 # +1 –∫–≤–æ—Ç–∞ –∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–≤–µ—Ç–æ–≤
            
            for item in resp['items']:
                replies.append({
                    '–ê–≤—Ç–æ—Ä': item['snippet']['authorDisplayName'],
                    '–¢–µ–∫—Å—Ç': item['snippet']['textDisplay'],
                    '–¢–∏–ø': '–û—Ç–≤–µ—Ç',
                    '–õ–∞–π–∫–∏': item['snippet']['likeCount']
                })
            
            progress_callback(len(replies))
            
            if 'nextPageToken' in resp:
                req = youtube.comments().list_next(req, resp)
            else: break
    except: pass
    return replies, cost

# --- –û–°–ù–û–í–ù–û–ô –ü–ê–†–°–ï–† –° –°–ß–ï–¢–ß–ò–ö–û–ú ---
def process_full_data(api_key, url, use_deep_scan):
    youtube = build('youtube', 'v3', developerKey=api_key)
    all_data = []
    file_name = "report.xlsx"
    total_cost = 0 # –°–ß–ï–¢–ß–ò–ö –ö–í–û–¢–´
    
    # –≠–ª–µ–º–µ–Ω—Ç—ã UI
    status_text = st.empty()
    bar = st.progress(0)

    if "v=" in url: v_id = url.split("v=")[1].split("&")[0]
    elif "youtu.be/" in url: v_id = url.split("youtu.be/")[1].split("?")[0]
    else: return [], "Bad Link", "", None, 0

    try:
        # 1. –ò–Ω—Ñ–æ –æ –≤–∏–¥–µ–æ (+1 –∫–≤–æ—Ç–∞)
        vid_req = youtube.videos().list(part="snippet", id=v_id).execute()
        total_cost += 1
        
        if vid_req['items']:
            title = vid_req['items'][0]['snippet']['title']
            file_name = f"{re.sub(r'[^\w\s-]', '', title)[:30]}.xlsx"
        else: return [], "Video not found", "", None, 1
        
        # 2. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è (0 –∫–≤–æ—Ç—ã)
        transcript = get_video_transcript(v_id)
        
        # 3. –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        req = youtube.commentThreads().list(part="snippet,replies", videoId=v_id, maxResults=100)
        
        fetched_count = 0
        while req:
            resp = req.execute()
            total_cost += 1 # +1 –∫–≤–æ—Ç–∞ –∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç—Ä–µ–¥–æ–≤
            
            for item in resp['items']:
                # –ì–ª–∞–≤–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç
                top = item['snippet']['topLevelComment']['snippet']
                all_data.append({
                    '–ê–≤—Ç–æ—Ä': top['authorDisplayName'], 
                    '–¢–µ–∫—Å—Ç': top['textDisplay'],
                    '–¢–∏–ø': '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π',
                    '–õ–∞–π–∫–∏': top['likeCount']
                })
                fetched_count += 1
                
                # –û—Ç–≤–µ—Ç—ã
                reply_count = item['snippet']['totalReplyCount']
                if reply_count > 0:
                    if use_deep_scan:
                        # –†–ï–ñ–ò–ú –ü–´–õ–ï–°–û–°–ê (–û—Ç–¥–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã)
                        status_text.text(f"üî• Deep Scan: –ö–∞—á–∞–µ–º –≤–µ—Ç–∫—É ({reply_count} –æ—Ç–≤–µ—Ç–æ–≤)... –ö–≤–æ—Ç–∞: {total_cost}")
                        replies, r_cost = get_replies_recursive(youtube, item['id'], lambda x: None)
                        all_data.extend(replies)
                        total_cost += r_cost
                        fetched_count += len(replies)
                    else:
                        # –≠–ö–û–ù–û–ú (–¢–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –ø—Ä–∏–ª–∏–ø–ª–æ)
                        if 'replies' in item:
                            for r in item['replies']['comments']:
                                all_data.append({
                                    '–ê–≤—Ç–æ—Ä': r['snippet']['authorDisplayName'], 
                                    '–¢–µ–∫—Å—Ç': r['snippet']['textDisplay'],
                                    '–¢–∏–ø': '–û—Ç–≤–µ—Ç',
                                    '–õ–∞–π–∫–∏': r['snippet']['likeCount']
                                })
                                fetched_count += 1
            
            bar.progress(min(fetched_count % 100, 100), text=f"–°–æ–±—Ä–∞–Ω–æ: {fetched_count} | –ü–æ—Ç—Ä–∞—á–µ–Ω–æ –∫–≤–æ—Ç—ã: {total_cost}")
            
            if 'nextPageToken' in resp:
                req = youtube.commentThreads().list_next(req, resp)
            else: break
            
    except Exception as e:
        return [], str(e), "", None, total_cost
    
    bar.empty()
    status_text.empty()
    return all_data, file_name, title, transcript, total_cost

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---
st.markdown("<h3 style='text-align: center;'>YouTubeComm</h3>", unsafe_allow_html=True)

raw_url = st.text_input("", placeholder="–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ...")

# –ü–ê–ù–ï–õ–¨ –£–ü–†–ê–í–õ–ï–ù–ò–Ø
with st.container(border=True):
    c1, c2 = st.columns(2)
    with c1:
        use_ai = st.toggle("ü§ñ –í–∫–ª—é—á–∏—Ç—å AI", value=False)
    with c2:
        deep_scan = st.toggle("‚ò¢Ô∏è Deep Scan (–í—Å–µ –æ—Ç–≤–µ—Ç—ã)", value=False, help="–ö–∞—á–∞–µ—Ç –≤—Å–µ –≤–µ—Ç–∫–∏. –¢—Ä–∞—Ç–∏—Ç –º–Ω–æ–≥–æ –∫–≤–æ—Ç—ã!")

# –ö–ù–û–ü–ö–ê –ò –°–ß–ï–¢–ß–ò–ö
btn_col, info_col = st.columns([1, 1])

with btn_col:
    start_btn = st.button("–ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É", type="primary", use_container_width=True)

with info_col:
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç—Ä–∞—Ç—ã –∫–≤–æ—Ç—ã
    if st.session_state['quota_used'] > 0:
        st.caption(f"üìâ –ü–æ—Ç—Ä–∞—á–µ–Ω–æ –∫–≤–æ—Ç—ã –∑–∞ —Ä–∞–∑: **{st.session_state['quota_used']}**")
        st.caption(f"–û—Å—Ç–∞—Ç–æ–∫ (–ø—Ä–∏–º–µ—Ä–Ω–æ): **{10000 - st.session_state['quota_used']}/10000**")

if start_btn:
    if not raw_url:
        st.warning("–ù–µ—Ç —Å—Å—ã–ª–∫–∏!")
    else:
        st.session_state['ai_verdict'] = None
        st.session_state['processed'] = False
        
        # 1. –ó–ê–ü–£–°–ö –ü–ê–†–°–ò–ù–ì–ê
        with st.spinner('–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö...'):
            data, fname, title, transcript, cost = process_full_data(API_KEY, raw_url, deep_scan)
            st.session_state['quota_used'] = cost # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å—Ö–æ–¥
        
        if data:
            # 2. EXCEL
            df = pd.DataFrame(data)
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                df.to_excel(writer, index=False)
            
            # 3. AI –ê–ù–ê–õ–ò–ó (–ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
            ai_text = None
            if use_ai:
                with st.spinner('AI –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏...'):
                    ai_text = get_ai_verdict(title, transcript, data, deep_scan)
                    st.session_state['ai_verdict'] = ai_text
            
            # 4. –û–¢–ü–†–ê–í–ö–ê
            quota_msg = f"–ü–æ—Ç—Ä–∞—á–µ–Ω–æ –∫–≤–æ—Ç—ã: {cost}"
            sent = send_to_telegram(buffer.getvalue(), fname, ai_text, quota_msg)
            
            if sent:
                st.success("‚úÖ –§–∞–π–ª –≤ Telegram!")
            st.session_state['processed'] = True
            st.rerun() # –û–±–Ω–æ–≤–ª—è–µ–º, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        else:
            st.error(f"–û—à–∏–±–∫–∞: {fname}")

# –†–ï–ó–£–õ–¨–¢–ê–¢ (–û—Å—Ç–∞–µ—Ç—Å—è –Ω–∞ —ç–∫—Ä–∞–Ω–µ)
if st.session_state['processed'] and st.session_state['ai_verdict']:
    st.divider()
    st.markdown(st.session_state['ai_verdict'])
