import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
import io
import re
import requests

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="YouTube AI Analyst", page_icon="üöÄ", layout="centered")

# --- –°–ï–ö–†–ï–¢–´ ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    TG_TOKEN = st.secrets["TELEGRAM_TOKEN"]
    TG_CHAT_ID = st.secrets["TELEGRAM_CHAT_ID"]
    GEMINI_KEY = st.secrets["GEMINI_API_KEY"]
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ Secrets: {e}")
    st.stop()

# --- –£–ú–ù–´–ô –ü–û–ò–°–ö –ú–û–î–ï–õ–ò ---
def find_best_model(api_key):
    """–ò—â–µ—Ç —Å–∞–º—É—é –Ω–æ–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å (Gemini 3 -> 2 -> 1.5 Pro)"""
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}"
    try:
        response = requests.get(url)
        if response.status_code != 200:
            return None, f"–û—à–∏–±–∫–∞ API: {response.status_code}"
            
        data = response.json()
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ —É–º–µ—é—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç
        all_models = [m['name'] for m in data.get('models', []) if 'generateContent' in m.get('supportedGenerationMethods', [])]
        
        if not all_models: return None, "–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –ø—É—Å—Ç"

        # –ü–†–ò–û–†–ò–¢–ï–¢–´ (–°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Å–∞–º—ã–µ –º–æ—â–Ω—ã–µ)
        # –ï—Å–ª–∏ –≤—ã–π–¥–µ—Ç Gemini 3, –æ–Ω –ø–æ–¥—Ö–≤–∞—Ç–∏—Ç—Å—è –ø–µ—Ä–≤—ã–º
        priority_keywords = ['gemini-3', 'gemini-2', 'gemini-1.5-pro', 'flash']
        
        for keyword in priority_keywords:
            # –ò—â–µ–º –º–æ–¥–µ–ª—å, –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∫–æ—Ç–æ—Ä–æ–π –µ—Å—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            found = next((m for m in all_models if keyword in m), None)
            if found:
                return found, None # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –ø–æ–ø–∞–≤—à—É—é—Å—è
        return all_models[0], None
        
    except Exception as e:
        return None, f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"

# --- –§–£–ù–ö–¶–ò–Ø –ê–ù–ê–õ–ò–ó–ê ---
def get_ai_summary(comments_list, model_name):
    if not comments_list: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."
    
    # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ—â–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    text_corpus = "\n".join([str(c['–¢–µ–∫—Å—Ç'])[:500] for c in comments_list[:80]])
    
    prompt = f"""
    –¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –º–µ–¥–∏–∞. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏.
    –î–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç—á–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (–∏—Å–ø–æ–ª—å–∑—É–π Markdown):
    
    1. üé≠ **–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–ª–∏–º–∞—Ç:** –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è.
    2. üî• **–û—Å—Ç—Ä—ã–µ —Ç–µ–º—ã:** –û —á–µ–º —Å–∞–º—ã–µ –∂–∞—Ä–∫–∏–µ —Å–ø–æ—Ä—ã?
    3. üëç **–ü–æ–∑–∏—Ç–∏–≤:** –ß—Ç–æ –∏–º–µ–Ω–Ω–æ —Ö–≤–∞–ª—è—Ç (—Ü–∏—Ç–∞—Ç—ã/—Ñ–∞–∫—Ç—ã).
    4. üëé **–ù–µ–≥–∞—Ç–∏–≤:** –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–µ—Ç–µ–Ω–∑–∏–∏.
    5. üß† **–í—ã–≤–æ–¥:** –°—Ç–æ–∏—Ç –ª–∏ –∞–≤—Ç–æ—Ä—É —á—Ç–æ-—Ç–æ –º–µ–Ω—è—Ç—å?
    
    –¢–µ–∫—Å—Ç: {text_corpus}
    """
    
    # model_name —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç "models/..."
    url = f"https://generativelanguage.googleapis.com/v1beta/{model_name}:generateContent?key={GEMINI_KEY}"
    
    try:
        response = requests.post(url, json={"contents": [{"parts": [{"text": prompt}]}]}, headers={"Content-Type": "application/json"})
        if response.status_code == 200:
            return response.json()['candidates'][0]['content']['parts'][0]['text']
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ({response.status_code}): {response.text}"
    except Exception as e:
        return f"–°–±–æ–π: {e}"

# --- –û–¢–ü–†–ê–í–ö–ê –í TELEGRAM (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø) ---
def send_full_report_to_telegram(file_data, file_name, ai_text):
    # 1. –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –§–ê–ô–õ
    url_doc = f"https://api.telegram.org/bot{TG_TOKEN}/sendDocument"
    try:
        requests.post(
            url_doc, 
            data={'chat_id': TG_CHAT_ID, 'caption': f"üìÇ –î–∞–Ω–Ω—ã–µ: {file_name}"}, 
            files={'document': (file_name, file_data)}
        )
    except: pass
    
    # 2. –ó–∞—Ç–µ–º –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –¢–ï–ö–°–¢ (–æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º, —á—Ç–æ–±—ã –≤–ª–µ–∑–ª–æ –≤—Å—ë)
    url_msg = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    try:
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ —Ç–µ–∫—Å—Ç –±–æ–ª—å—à–µ 4000 —Å–∏–º–≤–æ–ª–æ–≤ (—Ä–µ–¥–∫–æ, –Ω–æ –±—ã–≤–∞–µ—Ç)
        if len(ai_text) > 4000:
            ai_text = ai_text[:4000] + "\n...(–æ–±—Ä–µ–∑–∞–Ω–æ Telegram)..."
            
        requests.post(
            url_msg, 
            json={'chat_id': TG_CHAT_ID, 'text': ai_text, 'parse_mode': 'Markdown'}
        )
        return True
    except: return False

# --- –ü–ê–†–°–ò–ù–ì YOUTUBE ---
def get_video_id(url):
    if "v=" in url: return url.split("v=")[1].split("&")[0]
    elif "youtu.be/" in url: return url.split("youtu.be/")[1].split("?")[0]
    return None

def process_videos(api_key, urls):
    youtube = build('youtube', 'v3', developerKey=api_key)
    all_data = []
    file_name = "comments.xlsx"
    
    for i, url in enumerate(urls):
        v_id = get_video_id(url)
        if not v_id: continue
        try:
            vid_req = youtube.videos().list(part="snippet", id=v_id).execute()
            if vid_req['items']:
                title = vid_req['items'][0]['snippet']['title']
                file_name = f"{re.sub(r'[^\w\s-]', '', title)[:30]}.xlsx"
            
            req = youtube.commentThreads().list(part="snippet", videoId=v_id, maxResults=100)
            while req:
                resp = req.execute()
                for item in resp['items']:
                    top = item['snippet']['topLevelComment']['snippet']
                    all_data.append({'–ê–≤—Ç–æ—Ä': top['authorDisplayName'], '–¢–µ–∫—Å—Ç': top['textDisplay']})
                req = youtube.commentThreads().list_next(req, resp)
        except: pass
    return all_data, file_name

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---
st.title("YouTube AI Analyst 3.0 üöÄ")

# –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
with st.expander("üîå –°—Ç–∞—Ç—É—Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è", expanded=True):
    active_model, error = find_best_model(GEMINI_KEY)
    if active_model:
        st.success(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ—â–Ω–µ–π—à—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å: **{active_model}**")
    else:
        st.error(f"‚ùå –û—à–∏–±–∫–∞: {error}")

raw_urls = st.text_area("–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ:", height=100)

if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å", type="primary", disabled=(not active_model)):
    if not raw_urls:
        st.warning("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É")
    else:
        with st.spinner('–ß–∏—Ç–∞—é –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –¥—É–º–∞—é...'):
            data, fname = process_videos(API_KEY, raw_urls.split('\n'))
        
        if data:
            st.subheader("üìù –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞")
            summary = get_ai_summary(data, active_model)
            st.markdown(summary)
            
            # Excel
            df = pd.DataFrame(data)
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                df.to_excel(writer, index=False)
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞
            send_full_report_to_telegram(buffer.getvalue(), fname, summary)
            st.success("‚úÖ –û—Ç—á–µ—Ç –∏ —Ñ–∞–π–ª –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –≤ Telegram!")
            st.download_button("–°–∫–∞—á–∞—Ç—å Excel", buffer.getvalue(), fname)
